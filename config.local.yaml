# LLM Aggregator configuration
# All runtime behavior should be driven from here, not hard-coded in code.

host: "0.0.0.0"
port: 8888
# DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO
# Optional logging format string
# Example: Replace 'pathname' with 'filename' to get shorter output
log_format: "%(asctime)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
logger_overrides:
  httpx: WARNING

brain:
  # URL of the provider where the enrichment model is hosted
  base_url: "http://10.7.2.100:8080/v1"
  # Model used by the brain to enrich model metadata
  id: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS_llm-aggregator"
  # If set, send Authorization: Bearer <API-KEY> for calls to this port
  api_key: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS_llm-aggregator"
  # Maximum number of models to enrich in a single batch
  max_batch_size: 1

time:
  # Interval for fetching the list of models from all providers
  fetch_models_interval: 60
  # Timeout for fetching models from providers
  fetch_models_timeout: 10
  # Timeout for enriching models
  enrich_models_timeout: 120
  # for enrichment loop when queue is empty
  enrich_idle_sleep: 5

providers:
  - base_url: http://10.7.2.100:8080/v1
    internal_base_url: http://10.7.2.100:8080/v1
    # Optional bearer token for /v1/models calls to this provider
    api_key: null
  - base_url: http://10.7.2.100:8090/v1
  - base_url: http://10.7.2.100:11434/v1

# Additional websites where markdown model info can be fetched.
# Every template MUST contain {model_id} or the service will fail to start.
model_info_sources:
  - name: "HuggingFace.co"
    url_template: "https://huggingface.co/{model_id}"
  - name: "Ollama.com"
    url_template: "https://ollama.com/library/{model_id}"
