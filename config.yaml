# LLM Aggregator configuration
# All runtime behavior should be driven from here, not hard-coded in code.

host: "0.0.0.0"
port: 8888
# DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO
# Optional logging format string
# Example: Replace 'pathname' with 'filename' to get shorter output
log_format: "%(asctime)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
logger_overrides:
  httpx: WARNING

brain:
  # URL of the provider where the enrichment model is hosted
  base_url: "http://10.7.2.100:8080/v1"
  # Model used by the brain to enrich model metadata
  id: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS"
  # If set, send Authorization: Bearer <API-KEY> for calls to this port
  api_key: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS"
  # Maximum number of models to enrich in a single batch
  max_batch_size: 1

providers:
  - base_url: http://10.7.2.100:8080/v1
    internal_base_url: http://10.7.2.100:8080/v1
    # Optional bearer token for /v1/models calls to this provider
    api_key: null
  - base_url: http://10.7.2.100:8090/v1
  - base_url: http://10.7.2.100:11434/v1

# Additional websites where markdown model info can be fetched.
# Every template MUST contain {model_id} or the service will fail to start.
model_info_sources:
  - name: "HuggingFace.co"
    url_template: "https://huggingface.co/{model_id}"
  - name: "Ollama.com"
    url_template: "https://ollama.com/library/{model_id}"

time:
  # Interval for fetching the list of models from all providers
  fetch_models_interval: 60
  # Timeout for fetching models from providers
  fetch_models_timeout: 10
  # Timeout for enriching models
  enrich_models_timeout: 120
  # for enrichment loop when queue is empty
  enrich_idle_sleep: 5

ui:
  # Controls whether FastAPI serves static assets (either the bundled UI or a custom bundle).
  static_enabled: true
  # Optional custom UI bundle directory. Leave unset/null to use the built-in dashboard.
  # Custom bundles must contain a readable index.html at their root.
  # When static_enabled is false, nothing is served at "/" or "/static" even if a path is provided.
  custom_static_path: null

brain_prompts:
  system: |
    You are a strict JSON generator that analyzes a list of models and returns concise metadata.
    Only respond with a single JSON object, no markdown, no extra text.
  user: |
    Given the following JSON array 'models', generate detailed metadata for each LLM model.

    Before the JSON payload you may receive extra user messages like 'Model-Info for <id> from ExampleSource'. They contain markdown context scraped from trusted sources declared in the configuration. Use that information when relevant, but still obey every rule below.

    Return EXACTLY this JSON structure and nothing else (valid JSON):
    {
      "enriched": [
        {
          "id": "<exact id from input>",
          "base_url": "<exact base_url from input>",
          "internal_base_url": "<exact internal_base_url from input>",
          "summary": "<very short description of this specific model>",
          "types": ["llm"],
          "model_family": "<e.g. Gemma, Qwen, Llama, Mistral>",
          "context_size": <e.g. 4096, 8192, 32768, 131072>,
          "quant": "<e.g. Q4_K_M, Q5_K_M, Q8_0>",
          "param": "<e.g. 7B, 14B, 70B>"
        }
        // one entry per input model, in the same order
      ]
    }

    Field rules:
    - "types" is an array of tokens, each one of:
      ["llm", "vlm", "embedder", "reranker", "tts", "asr", "diarize", "cv", "image_gen"].
    - "model_family" is a short string like "Gemma", "Qwen", "Llama", "Mistral", etc.
    - "context_size" is an integer (best guess if unknown).
    - "quant" is the quantization string, if you can infer it.
    - "param" is the parameter size like "7B", "14B", "70B".

    General rules:
    - Include EVERY input model exactly once.
    - Preserve the exact id, base_url and internal_base_url from the input.
    - Use ONLY the allowed type tokens.
    - Keep summaries concise.
    - Never add extra top-level keys.
    - Never wrap your answer in markdown.

    Input 'models' follow (JSON array):
  model_info_prefix_template: "Model-Info for {model_id} from {provider_label}:"
