# LLM Aggregator configuration
# All runtime behavior should be driven from here, not hard-coded in code.

host: "0.0.0.0"
port: 8888
log_level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

brain:
  # URL of the provider where the enrichment model is hosted
  base_url: "http://10.7.2.100:8080/v1"
  # Model used by the brain to enrich model metadata
  id: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS"
  # If set, send Authorization: Bearer <API-KEY> for calls to this port
  api_key: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS"
  # Maximum number of models to enrich in a single batch
  max_batch_size: 5

time:
  # Interval for fetching the list of models from all providers
  fetch_models_interval: 60
  # Timeout for fetching models from providers
  fetch_models_timeout: 10
  # Timeout for enriching models
  enrich_models_timeout: 120
  # for enrichment loop when queue is empty
  enrich_idle_sleep: 5

providers:
  - base_url: http://10.7.2.100:8080/v1
    internal_base_url: http://10.7.2.100:8080/v1
  - base_url: http://10.7.2.100:8090/v1
  - base_url: http://10.7.2.100:11434/v1
