# LLM Aggregator configuration
# All runtime behavior should be driven from here, not hard-coded in code.

host: "0.0.0.0"
port: 8888
# DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO
# Optional logging format string
# Example: Replace 'pathname' with 'filename' to get shorter output
log_format: "%(asctime)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
logger_overrides:
  httpx: WARNING

brain:
  # URL of the provider where the enrichment model is hosted
  base_url: "http://10.7.2.100:11434/v1"
  # Model used by the brain to enrich model metadata
  id: "qwen3:8b"
  # If set, send Authorization: Bearer <API-KEY> for calls to this port
  api_key: "qwen3:8b"
  # Maximum number of models to enrich in a single batch
  max_batch_size: 1

providers:
  llama.cpp:
    # Public URL of the provider for users
    base_url: http://10.7.2.100:8080/v1
    files_size_gatherer:
      path: src/llm_aggregator/scripts/files-size-llama-cpp.sh
      base_path: "/Volumes/OWCUltra/ollama/models/"
  nexa.ai:
    base_url: http://10.7.2.100:8090/v1
    # Optional internal url for server-to-server calls
    internal_base_url: http://10.7.2.100:8090/v1
    # Optional bearer token for /v1/models calls to this provider
    api_key: null
  Ollama:
    base_url: http://10.7.2.100:11434/v1
    # Optional files size gatherer configuration
    files_size_gatherer:
      # Script is invoked as: <path> <base_path> <full_model_name>
      # Packaged helper script (relative to the repo / package)
      path: src/llm_aggregator/scripts/files-size-50GiB.sh
      base_path: "/var/lib/ollama/models"
      # Optional per-gatherer timeout in seconds (default: 15)
      timeout_seconds: 15

# Additional websites where markdown model info can be fetched.
# Every template MUST contain {model_id} or the service will fail to start.
model_info_sources:
  - name: "HuggingFace.co"
    url_template: "https://huggingface.co/{model_id}"
  - name: "Ollama.com"
    url_template: "https://ollama.com/library/{model_id}"

time:
  # Interval for fetching the list of models from all providers
  fetch_models_interval: 60
  # Timeout for fetching models from providers
  fetch_models_timeout: 10
  # Timeout for enriching models
  enrich_models_timeout: 300
  # for enrichment loop when queue is empty
  enrich_idle_sleep: 5
  # TTL for cached website markdown scraped from external sources
  website_markdown_cache_ttl: 604800

ui:
  # Controls whether FastAPI serves static assets (either the bundled UI or a custom bundle).
  static_enabled: true
  # Optional custom UI bundle directory. Leave unset/null to use the built-in dashboard.
  # Custom bundles must contain a readable index.html at their root.
  # When static_enabled is false, nothing is served at "/" or "/static" even if a path is provided.
  custom_static_path: null

brain_prompts:
  system: |
    You are a strict JSON generator that analyzes a list of models and returns concise metadata.
    Only respond with a single JSON object, no markdown, no extra text.
  user: |
    Given the following JSON array 'models', generate detailed metadata for each LLM model.

    Before the JSON payload you may receive extra user messages like 'Model-Info for <id> from ExampleSource'. They contain markdown context scraped from trusted sources declared in the configuration. Use that information when relevant, but still obey every rule below.

    Return EXACTLY this JSON structure and nothing else (valid JSON):
    [
      {
        "id": "<exact id from input>",
        "provider": "<exact provider from input>",
        "summary": "<very short description of this specific model>",
        "types": ["llm"],
        "model_family": "<e.g. Gemma, Qwen, Llama, Mistral>",
        "context_size": <e.g. 4096, 8192, 32768, 131072>,
        "quant": "<e.g. Q4_K_M, Q5_K_M, Q8_0>",
        "param": "<e.g. 7B, 14B, 70B>"
      }
      // one entry per input model, in the same order
    ]

    Field rules:
    - "provider" is the provider name from the input.
    - "types" is an array of tokens, each one of:
      ["llm", "vlm", "embedder", "reranker", "tts", "asr", "diarize", "cv", "image_gen"].
    - "model_family" is a short string like "Gemma", "Qwen", "Llama", "Mistral", etc.
    - "context_size" is an integer (best guess if unknown).
    - "quant" is the quantization string, if you can infer it., Common values are:
      ["Q2", "Q3", "Q4", "Q5", "Q6", "Q8", "FP16", "FP32"].
    - "param" is the parameter size like "7B", "14B", "70B".

    General rules:
    - Include EVERY input model exactly once.
    - Preserve the exact id and provider from the input.
    - Use ONLY the allowed type tokens.
    - Keep summaries concise.
    - Never add extra top-level keys.
    - Never wrap your answer in markdown.

    Input 'models' follow (JSON array):
  model_info_prefix_template: "Model-Info for {model_id} from {provider_label}:"
