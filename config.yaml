# LLM Aggregator configuration
# All runtime behavior should be driven from here, not hard-coded in code.

brain:
  host: "http://10.7.2.100"
  # Port of the provider where the enrichment model is hosted
  port: 8080
  # Model used by the brain to enrich model metadata
  model_id: "unsloth/GLM-4.6-GGUF:UD-IQ2_XXS"
  # If true, send Authorization: Bearer <model_id> for calls to this port
  use_bearer_model_id: true
  # Maximum number of models to enrich in a single batch
  max_batch_size: 5

refresh:
  # Interval for refreshing the list of models from all providers
  interval_seconds: 60

timeout:
  # Timeout for fetching models from providers
  fetch_models_seconds: 10
  # Timeout for enriching models
  enrich_models_seconds: 60

providers:
  - base_url: http://10.7.2.100
    port: 8080
  - base_url: http://10.7.2.100
    port: 8090
  - base_url: http://10.7.2.100
    port: 11434

# How long /api/models responses may be served from the last computed snapshot (in seconds)
cache_ttl_seconds: 300
